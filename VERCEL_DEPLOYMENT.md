# ðŸš€ Deploy Bull Bot to Vercel

## Why Vercel Only?

Instead of the complex AWS + Cyclic + Vercel setup, we can deploy **everything** to Vercel:
- âœ… Frontend (Vite/React) â†’ Vercel Static
- âœ… Backend (Flask API) â†’ Vercel Serverless Functions
- âœ… No need for Express server
- âœ… No need for AWS EC2
- âœ… 100% free for MVP/development

---

## Prerequisites

Before deploying, ensure you have:

1. âœ… **Pinecone Index Created**
   - Index name: `bullbot`
   - Dimensions: 384
   - Metric: cosine
   - Already populated with data

2. âœ… **OpenAI API Key Working**
   - Has available credits
   - Tested successfully

3. âœ… **Vercel Account**
   - Sign up at https://vercel.com (free)
   - Can use GitHub login

4. âœ… **Vercel CLI Installed**
   ```bash
   npm install -g vercel
   ```

---

## Local Testing (Simplified Setup)

Before deploying, test locally with the new simplified architecture:

### Start the Flask API (No Express needed!)

```bash
cd /Users/maazinshaikh/Desktop/Mac/Programs/AskRocky/bullbot
python api/index.py
```

Server will run on `http://localhost:8000`

### Start the Vite Client

```bash
cd client
npm run dev
```

Client will run on `http://localhost:5173`

### Test It

Open http://localhost:5173 and ask "What is USF?"

---

## Deploy to Vercel

### Step 1: Login to Vercel

```bash
cd /Users/maazinshaikh/Desktop/Mac/Programs/AskRocky/bullbot
vercel login
```

### Step 2: Set Environment Variables

You need to add your API keys as environment secrets in Vercel:

```bash
# Add Pinecone API key
vercel env add PINECONE_API

# When prompted, paste your key (example placeholder):
# pcsk_your_pinecone_key_here

# Add OpenAI API key
vercel env add OPENAI_API_KEY

# When prompted, paste your key (example placeholder):
# sk-your-openai-key-here
```

For each variable, when asked which environments:
- Select: **Production, Preview, and Development** (press Space to select all, Enter to confirm)

### Step 3: Deploy!

```bash
vercel
```

Follow the prompts:
- **Set up and deploy?** â†’ Yes
- **Which scope?** â†’ Your account
- **Link to existing project?** â†’ No
- **Project name?** â†’ bullbot (or your choice)
- **Directory with code?** â†’ ./ (current directory)
- **Override settings?** â†’ No

Vercel will:
1. Build your frontend
2. Deploy your Flask backend as serverless functions
3. Give you a live URL (e.g., `https://bullbot.vercel.app`)

### Step 4: Verify Deployment

Once deployed, Vercel will give you a URL. Test it:

1. Open the URL in your browser
2. Try asking questions about USF
3. Check the Vercel logs if there are issues:
   ```bash
   vercel logs
   ```

---

## Environment Variables Configuration

The following environment variables are read from `flaskServer/config.py`:

- `PINECONE_API` - Your Pinecone API key
- `PINECONE_ENV` - Set to `us-east-1` (in config.py)
- `PINECONE_INDEX` - Set to `bullbot` (in config.py)
- `OPENAI_API_KEY` - Your OpenAI API key

You only need to set `PINECONE_API` and `OPENAI_API_KEY` in Vercel, the others are hardcoded in config.py.

---

## Troubleshooting

### Issue: "Module not found" errors

**Solution:** Check that `requirements.txt` is in the root directory with all dependencies.

### Issue: "Pinecone connection failed"

**Solution:** 
1. Verify your Pinecone API key is set correctly in Vercel
2. Check that the index name is exactly `bullbot`
3. Ensure the index has data uploaded

### Issue: "OpenAI quota exceeded"

**Solution:** 
1. Add credits to your OpenAI account
2. Update the API key in Vercel environment variables

### Issue: Cold start is slow

**Solution:** This is normal for Vercel serverless functions. First request may take 10-30 seconds as it:
- Loads the embedding model
- Connects to Pinecone
- Initializes OpenAI

Subsequent requests will be much faster (1-3 seconds).

### Issue: Function timeout

**Solution:** Vercel free tier has a 10-second timeout. If needed:
1. Upgrade to Pro plan ($20/month) for 60-second timeout
2. Or optimize by pre-loading models (already done in the code)

---

## What Changed From Original Architecture?

### Old (3-server setup):
```
Client (Vercel) â†’ Express Server (Cyclic) â†’ Flask Server (AWS EC2)
```

### New (Vercel-only):
```
Client (Vercel Static) â†’ Flask API (Vercel Serverless)
```

### Benefits:
- âœ… Simpler deployment
- âœ… No server management
- âœ… Free hosting
- âœ… Automatic HTTPS
- âœ… Global CDN
- âœ… Auto-scaling

---

## Project Structure for Vercel

```
bullbot/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ index.py              # Flask API (Vercel serverless function)
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ script.js
â”‚   â”œâ”€â”€ style.css
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.production       # Points to /api/chat
â”œâ”€â”€ flaskServer/
â”‚   â””â”€â”€ config.py             # Config imported by api/index.py
â”œâ”€â”€ vercel.json               # Vercel configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ VERCEL_DEPLOYMENT.md      # This file
```

---

## Monitoring & Logs

### View Logs
```bash
vercel logs --follow
```

### View Deployment in Dashboard
```bash
vercel open
```

Or go to: https://vercel.com/dashboard

---

## Costs

### Free Tier Includes:
- âœ… Unlimited static deployments
- âœ… 100GB bandwidth/month
- âœ… Serverless function execution (fair use)
- âœ… Automatic HTTPS
- âœ… Preview deployments for Git branches

### You Pay For:
- OpenAI API usage (~$0.002 per request with GPT-3.5-turbo)
- Pinecone (free tier: 1 index, 100K vectors)

**Estimated cost for MVP:** $0-5/month depending on usage

---

## Next Steps After Deployment

1. **Custom Domain** (Optional)
   ```bash
   vercel domains add yourdomain.com
   ```

2. **Analytics** (Optional)
   - Enable Vercel Analytics in dashboard
   - Track page views and performance

3. **Continuous Deployment**
   - Push to GitHub
   - Connect repo to Vercel
   - Auto-deploy on every push

4. **Monitor Usage**
   - Check Vercel dashboard for function invocations
   - Monitor OpenAI usage at platform.openai.com
   - Check Pinecone dashboard for query counts

---

## Quick Reference

### Deploy Updates
```bash
vercel --prod
```

### Rollback to Previous Deployment
```bash
vercel rollback
```

### View All Deployments
```bash
vercel ls
```

### Remove Project
```bash
vercel remove bullbot
```

---

## Summary

With Vercel-only deployment, you get:
- ðŸš€ **Simple deployment:** One command
- ðŸ’° **Free hosting:** No AWS bills
- âš¡ **Fast performance:** Global CDN
- ðŸ”’ **Secure:** Automatic HTTPS
- ðŸ“Š **Easy monitoring:** Vercel dashboard

Perfect for MVP and can scale to production!
